{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import model\n",
    "\n",
    "def top_k_logits(logits, k):\n",
    "    if k == 0:\n",
    "        # no truncation\n",
    "        return logits\n",
    "\n",
    "    def _top_k():\n",
    "        values, _ = tf.nn.top_k(logits, k=k)\n",
    "        min_values = values[:, -1, tf.newaxis]\n",
    "        return tf.where(\n",
    "            logits < min_values,\n",
    "            tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "    return tf.cond(\n",
    "       tf.equal(k, 0),\n",
    "       lambda: logits,\n",
    "       lambda: _top_k(),\n",
    "    )\n",
    "\n",
    "\n",
    "def top_p_logits(logits, p):\n",
    "    \"\"\"Nucleus sampling\"\"\"\n",
    "    batch, _ = logits.shape.as_list()\n",
    "    sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
    "    cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "    indices = tf.stack([\n",
    "        tf.range(0, batch),\n",
    "        # number of indices to include\n",
    "        tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
    "    ], axis=-1)\n",
    "    min_values = tf.gather_nd(sorted_logits, indices)\n",
    "    return tf.where(\n",
    "        logits < min_values,\n",
    "        tf.ones_like(logits) * -1e10,\n",
    "        logits,\n",
    "    )\n",
    "\n",
    "\n",
    "def sample_sequence(*, hparams, length, start_token=None, batch_size=None, context=None, temperature=1, top_k=0, top_p=1):\n",
    "    if start_token is None:\n",
    "        assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "    else:\n",
    "        assert context is None, 'Specify exactly one of start_token and context!'\n",
    "        context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "    def step(hparams, tokens, past=None):\n",
    "        lm_output = model.model(hparams=hparams, X=tokens, past=past, reuse=tf.AUTO_REUSE)\n",
    "\n",
    "        logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "        presents = lm_output['present']\n",
    "        presents.set_shape(model.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'presents': presents,\n",
    "        }\n",
    "\n",
    "    with tf.name_scope('sample_sequence'):\n",
    "        def body(past, prev, output):\n",
    "            next_outputs = step(hparams, prev, past=past)\n",
    "            logits = next_outputs['logits'][:, -1, :]  / tf.to_float(temperature)\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            logits = top_p_logits(logits, p=top_p)\n",
    "            samples = tf.multinomial(logits, num_samples=1, output_dtype=tf.int32)\n",
    "            return [\n",
    "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "                samples,\n",
    "                tf.concat([output, samples], axis=1)\n",
    "            ]\n",
    "\n",
    "        past, prev, output = body(None, context, context)\n",
    "\n",
    "        def cond(*args):\n",
    "            return True\n",
    "\n",
    "        _, _, tokens = tf.while_loop(\n",
    "            cond=cond, body=body,\n",
    "            maximum_iterations=length - 1,\n",
    "            loop_vars=[\n",
    "                past,\n",
    "                prev,\n",
    "                output\n",
    "            ],\n",
    "            shape_invariants=[\n",
    "                tf.TensorShape(model.past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "            ],\n",
    "            back_prop=False,\n",
    "        )\n",
    "\n",
    "        return tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
